import tempfile
import streamlit as st 

from streamlit_chat import message
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import CTransformers
from langchain.chains import ConversationalRetrievalChain

def load_llm(type=0, temperature = 0.0, max_new_tokens=512):
    """Load the selected llm model based on given type int value

    Args:
        type (int, optional): 
            Type of Llama2 Model. 
            Defaults to 0.
        temperature (float, optional): 
            Creativity of the model as higher value produce more creative 
            text while lower produce more predictable text. 
            Defaults to 0.0.
        max_new_tokens (int, optional): 
            The maximum numbers of tokens to generate, ignore the current 
            number of tokens. 
            Defaults to 512.

    Returns:
        llm: An LLM model Object type
    """

    ## Any added model should include here
    model_dict = {
        0:{
            "model":"TheBloke/Llama-2-7B-Chat-GGUF", 
            "model_file":"llama-2-7b-chat.Q4_K_M.gguf"
        },
        1:{
            "model":"TheBloke/Llama-2-7B-Chat-GGUF", 
            "model_file":"llama-2-7b-chat.Q5_K_M.gguf"
        },
        2:{
            "model":"TheBloke/Llama-2-7B-Chat-GGUF", 
            "model_file":"llama-2-7b-chat.Q6_K.gguf"
        },
        3:{
            "model":"TheBloke/Llama-2-7B-Chat-GGUF", 
            "model_file":"llama-2-7b-chat.Q8_0.gguf"
        },
        4:{
            "model":"TheBloke/Llama-2-13B-Chat-GGUF", 
            "model_file":"llama-2-13b-chat.Q4_K_M.gguf"
        },
        5:{
            "model":"TheBloke/Llama-2-13B-Chat-GGUF", 
            "model_file":"llama-2-13b-chat.Q5_K_M.gguf"
        },
        6:{
            "model":"TheBloke/Llama-2-13B-Chat-GGUF", 
            "model_file":"llama-2-13b-chat.Q6_K.gguf"
        },
        7:{
            "model":"TheBloke/Llama-2-13B-Chat-GGUF", 
            "model_file":"llama-2-13b-chat.Q8_0.gguf"
        }
    }

    llm = CTransformers(
        model=model_dict[type]["model"], 
        model_file=model_dict[type]["model_file"], 
        model_type="llama",
        max_new_tokens = max_new_tokens,
        temperature = temperature
    )

    return llm

def embedding_model(type=0):
    """Measure of relatedness of text strings

    Args:
        type (int, optional): 
            Type of given Embedding Models. 
            Defaults to 0.

    Returns:
        Embedding: An Embedding model Object type
    """

    model_kwargs = {'device': 'cpu'}

    if type == 0:
        model_name = 'sentence-transformers/all-MiniLM-L6-v2'
        embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs
                )

    elif type == 1:
        model_name = "BAAI/bge-small-en"
        encode_kwargs = {'normalize_embeddings': True}
        embeddings = HuggingFaceBgeEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )

    elif type == 2:
        model_name = "hkunlp/instructor-xl"
        embeddings = HuggingFaceInstructEmbeddings(
            model_name=model_name, 
            model_kwargs=model_kwargs
            )

    return embeddings

def conversational_chat(chain, query):
    """_summary_

    Args:
        chain (langchain.chains Object): 
            A Chain object to perform retrieval process
        query (str): 
            A string messages to be chained

    Returns:
        str: Message generated by LangChain model
    """
    result = chain({"question": query, "chat_history": st.session_state['history']})
    st.session_state['history'].append((query, result["answer"]))

    return result["answer"]

def main():
    ## Help to relieve RAM 
    _vsdb_path = './vectorstore/db_faiss'

    ## Page Configuration
    st.set_page_config(
        page_title="Simple RoboAdvisor for DEMO",
        page_icon=":robot:"
    )

    st.header("üê¶ KaggleX DEMO - RoboAdvisor üìä")

    ## Get Data
    _import_file = st.sidebar.file_uploader("Upload your Data", type="csv")

    ## container for chat history
    _chat_container = st.container()
    # #container for user input
    _user_container = st.container()

    ## Check if file is uploaded yet
    if _import_file:

        if 'history' not in st.session_state:
            st.session_state['history'] = []

        if 'generated' not in st.session_state:
            st.session_state['generated'] = ["Hello! Try asking me anything about " + _import_file.name]

        if 'past' not in st.session_state:
            st.session_state['past'] = ["Hello ChatBot!"]

        #use tempfile because CSVLoader only accepts a file_path
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            tmp_file.write(_import_file.getvalue())
            tmp_file_path = tmp_file.name

        print(tmp_file_path)

        ## CSV LOADER
        loader = CSVLoader(
            file_path =tmp_file_path, 
            encoding  ="utf-8", 
            csv_args={'delimiter': ','}
            )

        data = loader.load()

        ## Load Embedding Model to VectorDB
        db = FAISS.from_documents(data, embedding_model(type=0))
        db.save_local(_vsdb_path)

        ## Load model & initiate Chain
        llm = load_llm(type=7)
        chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever())

        with _user_container:
            with st.form(key='my_form', clear_on_submit=True):
                
                user_input = st.text_input("Query:", placeholder="Talk to your csv data here (:", key='input')
                submit_button = st.form_submit_button(label='Send')
                
            if submit_button and user_input:
                output = conversational_chat(chain, user_input)
                
                st.session_state['past'].append(user_input)
                st.session_state['generated'].append(output)

        if st.session_state['generated']:
            with _chat_container:
                for i in range(len(st.session_state['generated'])):
                    message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style="big-smile")
                    message(st.session_state["generated"][i], key=str(i), avatar_style="thumbs")

    else:
        st.text("Please upload a csv file to the left to kick-ON this ChatBot...")
        st.text("Waiting for CSV file!!")

if __name__ == "__main__":
    main()     